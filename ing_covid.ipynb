{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fcba9c-005f-412e-9d04-c31ddf9e18f0",
   "metadata": {},
   "source": [
    "### Ingestas desde archivos CSV a la capa de Bronce en formato .parquet\n",
    "Se utilizan las clases para poder realizar las ingestas con Spark y/o Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665fd18c-a174-4de3-b692-e3ec059da0d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from medallion_pkg.dataLoader import SparkDataLoader\n",
    "from medallion_pkg.dataLoader import PandasDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207a4fb-40c3-4631-9800-373822df89f2",
   "metadata": {},
   "source": [
    "Carga el archivo csv a parquet utilizando la clase SparkDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8d9e56-a37c-45a3-88d6-e413d002a333",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 19:22:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark guardó el DataFrame como Parquet en: /opt/spark/scripts/data/bronce/covid_19_clean_complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD count: 49068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 19:22:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------+---------+----------+----------+---------+------+---------+------+---------------------+\n",
      "|Province/State              |Country/Region     |Lat      |Long      |Date      |Confirmed|Deaths|Recovered|Active|WHO Region           |\n",
      "+----------------------------+-------------------+---------+----------+----------+---------+------+---------+------+---------------------+\n",
      "|NULL                        |Afghanistan        |33.93911 |67.709953 |2020-01-22|0        |0     |0        |0     |Eastern Mediterranean|\n",
      "|NULL                        |Albania            |41.1533  |20.1683   |2020-01-22|0        |0     |0        |0     |Europe               |\n",
      "|NULL                        |Algeria            |28.0339  |1.6596    |2020-01-22|0        |0     |0        |0     |Africa               |\n",
      "|NULL                        |Andorra            |42.5063  |1.5218    |2020-01-22|0        |0     |0        |0     |Europe               |\n",
      "|NULL                        |Angola             |-11.2027 |17.8739   |2020-01-22|0        |0     |0        |0     |Africa               |\n",
      "|NULL                        |Antigua and Barbuda|17.0608  |-61.7964  |2020-01-22|0        |0     |0        |0     |Americas             |\n",
      "|NULL                        |Argentina          |-38.4161 |-63.6167  |2020-01-22|0        |0     |0        |0     |Americas             |\n",
      "|NULL                        |Armenia            |40.0691  |45.0382   |2020-01-22|0        |0     |0        |0     |Europe               |\n",
      "|Australian Capital Territory|Australia          |-35.4735 |149.0124  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|New South Wales             |Australia          |-33.8688 |151.2093  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|Northern Territory          |Australia          |-12.4634 |130.8456  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|Queensland                  |Australia          |-27.4698 |153.0251  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|South Australia             |Australia          |-34.9285 |138.6007  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|Tasmania                    |Australia          |-42.8821 |147.3272  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|Victoria                    |Australia          |-37.8136 |144.9631  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|Western Australia           |Australia          |-31.9505 |115.8605  |2020-01-22|0        |0     |0        |0     |Western Pacific      |\n",
      "|NULL                        |Austria            |47.5162  |14.5501   |2020-01-22|0        |0     |0        |0     |Europe               |\n",
      "|NULL                        |Azerbaijan         |40.1431  |47.5769   |2020-01-22|0        |0     |0        |0     |Europe               |\n",
      "|NULL                        |Bahamas            |25.025885|-78.035889|2020-01-22|0        |0     |0        |0     |Americas             |\n",
      "|NULL                        |Bahrain            |26.0275  |50.55     |2020-01-22|0        |0     |0        |0     |Eastern Mediterranean|\n",
      "+----------------------------+-------------------+---------+----------+----------+---------+------+---------+------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_loader_covid_19_clean_complete = SparkDataLoader('/opt/spark/scripts/source/covid_19_clean_complete.csv', '/opt/spark/scripts/data/bronce/covid_19_clean_complete')\n",
    "spark_loader_covid_19_clean_complete.load_and_save_parquet()\n",
    "rdd = spark_loader_covid_19_clean_complete.load_csv_as_rdd()\n",
    "#  Leer el parquet ingestado\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read parquet\") \\\n",
    "    .getOrCreate()\n",
    "df_bronce = spark.read.parquet(\"/opt/spark/scripts/data/bronce/covid_19_clean_complete\")\n",
    "df_bronce.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6b4b9-a353-4e9e-93bc-f1f8a87dc5e0",
   "metadata": {},
   "source": [
    "Carga el archivo csv a parquet utilizando la clase PandasDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1acfb-3ff1-4ae0-add5-4298b74aee26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pandas_loader = PandasDataLoader('/opt/spark/scripts/source/usa_county_wise.csv', '/opt/spark/scripts/data/bronce/usa_county_wise.parquet')\n",
    "pandas_loader.load_and_save_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd008ee-c5dc-4673-81c6-747bc5023e6b",
   "metadata": {},
   "source": [
    "Esto sería una alternativa para mantener la ingesta parametrizada en un archivo; de esta manera se puede ir agregando mas input a las ingestas mediante este archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbc7e8-72df-43e0-93e8-d18e73f14b21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path_file = \"/opt/spark/scripts/source/paths_ingesta.txt\"\n",
    "rdd_paths = spark.sparkContext.textFile(path_file)\n",
    "\n",
    "# Parsear las rutas\n",
    "paths = rdd_paths.map(lambda line: line.strip().split(\",\")) \\\n",
    "                 .filter(lambda x: len(x) == 2) \\\n",
    "                 .collect()\n",
    "\n",
    "# Procesar cada archivo\n",
    "for origen, destino in paths:\n",
    "    print(f\"Procesando: {origen} → {destino}\")\n",
    "    spark_loader = SparkDataLoader(origen, destino)\n",
    "    spark_loader.load_and_save_parquet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
